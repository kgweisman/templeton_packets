---
title: 'SC data entry: Packets 1-3'
subtitle: 'Checking double-entry (updated with data from 2018-01-31)'
output:
  pdf_document: default
  html_document:
    df_print: paged
  html_notebook: default
---

This is a document describing how we check accuracy for packets that were double-entered (entered more than once).

# Setup

```{r}
# load packages
library(tidyverse)

# load data
d_all <- read.csv("//Users/kweisman/Documents/Research (Stanford)/Projects/Templeton Grant/DATA WRANGLING/templeton_packets/packets123/packets123_data.csv")[-1]
question_key <- read.csv("//Users/kweisman/Documents/Research (Stanford)/Projects/Templeton Grant/DATA WRANGLING/templeton_packets/packets123/packets123_question_key_R.csv")[1]
# these "-1"s are to get rid of that extra column at the beginning

# make custom min and max functions
min_fun <- function(x) ifelse( !all(is.na(x)), min(x, na.rm=T), NA)
max_fun <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)
```

# Checking numerical data

Let's first deal with numerical data, because that's a whole lot easier!

```{r}
# figure out which variables are numeric
cat_vars <- d_all %>%
  select_if(is.numeric) %>%
  names()

# make a separate dataframe for numerical data
d_num <- d_all %>%
  select(ctry, subj, packet, version, batc, entr, date, c(cat_vars))
```

There are `r length(d_num)` numeric variables (!) that we can check.

I'll check them in two ways for now, using the following logic:

- If the entries are the same, the minimum, maximum, and mean should all be identical
- If the entries are the same, the standard deviation should be 0

If any pair of entries fails either or both of these tests, I'll flag it as having a problem:

```{r}
d_num_check <- d_num %>%
  gather(question, response, -c(ctry, subj, packet, version, batc, entr, date)) %>%
  group_by(subj, packet, version, question) %>%
  summarise(count = n(),
            min = min_fun(response),
            max = max_fun(response),
            mean = mean(response, na.rm = T),
            sd = sd(response, na.rm = T)) %>%
  mutate(min_max_mean_mismatch = ifelse(min == max & min == mean, FALSE, TRUE),
         sd_nonzero = ifelse(sd %in% c(0, NA), FALSE, TRUE),
         probs = min_max_mean_mismatch | sd_nonzero)

# look at full check (uncomment to look)
# d_num_check

# count up problems and calculate error rate
error_rates_num <- d_num_check %>%
  ungroup() %>%
  count(probs) %>%
  filter(!is.na(probs)) %>%
  mutate(proportion = round(n/sum(n), 5)) %>%
  data.frame()

error_rates_num
```

Whoohoo!!  Error rate seems to be SUPER low: `r round(error_rates_num[error_rates_num$probs == TRUE,"proportion"], 3)*100`%! WOW.

What are the errors that did happen? We could go look at them by looking up the subject ID (`subj`), packet (`packet`), version (`version`), and question (`question`) in the following dataframe:

```{r}
errors_num <- d_num_check %>%
  filter(probs == TRUE) %>%
  data.frame() %>%
  select(subj, packet, version, question) %>%
  left_join(d_num %>% 
              gather(question, response, 
                     -c(ctry, subj, packet, version, batc, entr, date))) %>%
  ungroup() %>%
  distinct(ctry, subj, packet, version, batc, entr, date, question, response) %>%
  arrange(subj, packet, version, question, batc)

# look at full errors (uncomment to look, or look at errors_num.csv)
# errors_num
```

```{r}
# get counts per country to get proportions later
all_num <- d_num_check %>%
  data.frame() %>%
  select(subj, packet, version, question) %>%
  left_join(d_num %>% 
              gather(question, response, 
                     -c(ctry, subj, packet, version, batc, entr, date))) %>%
  ungroup() %>%
  distinct(ctry, subj, packet, version, batc, entr, date, question, response) %>%
  arrange(subj, packet, version, question, batc)
```


Let's explore this just a little more (*NOTE: The counts and proportions here won't really match up with the official error rates calculated above - they kind of double-count stuff. I'll work on this in the future - for now, I think the relative proportions are still meaningful*):

- Which countries had the most errors?

```{r}
errors_num %>%
  count(ctry) %>%
  complete(ctry, fill = list(n = 0)) %>%
  full_join(all_num %>% count(ctry) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

- Which packets/versions?

```{r}
errors_num %>%
  count(packet, version) %>%
  complete(packet, version, fill = list(n = 0)) %>%
  full_join(all_num %>% count(packet, version) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

- Which subjects?

```{r}
errors_num %>%
  count(subj) %>%
  complete(subj, fill = list(n = 0)) %>%
  full_join(all_num %>% count(subj) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

Which questions?

```{r}
errors_num %>%
  count(question) %>%
  complete(question, fill = list(n = 0)) %>%
  full_join(all_num %>% count(question) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

Which data-enterers?
*NOTE: Of course, one person in any given mis-match is probably correct, so these aren't real error-rates - but the relative proportions might be useful.*

```{r}
errors_num %>%
  count(entr) %>%
  complete(entr, fill = list(n = 0)) %>%
  full_join(all_num %>% count(entr) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

# Checking categorical data

Now let's deal with categorical data. Thought this would be hard, but it's actually not... it will be hard to reconcile, but not to detect mismatches!

```{r}
# figure out which variables are factor
factor_vars <- d_all %>%
  select_if(is.factor) %>%
  names()

char_vars <- d_all %>%
  select_if(is.character) %>%
  names()

log_vars <- d_all %>%
  select_if(is.logical) %>%
  names()

cat_vars <- c(factor_vars, char_vars, log_vars) %>% unique()

# make a separate dataframe for categorical data
d_cat <- d_all %>%
  select(ctry, subj, packet, version, batc, entr, date, c(cat_vars))
```

There are `r length(d_cat)` factor variables (!) that we can check.

I'll check them in one way for now, using the following logic:

- If the entries are the same, there should be exactly one answer per question

If any pair of entries fails either or both of these tests, I'll flag it as having a problem:

```{r}
d_cat_check <- d_cat %>%
  gather(question, response, -c(ctry, subj, packet, version, batc, entr, date)) %>%
  group_by(ctry, subj, packet, version, question) %>%
  distinct(response) %>%
  count() %>%
  ungroup() %>%
  mutate(probs = ifelse(n > 1, TRUE, FALSE))

# look at full check (uncomment to look)
# d_cat_check

# count up problems and calculate error rate
error_rates_cat <- d_cat_check %>%
  ungroup() %>%
  count(probs) %>%
  rename(n = nn) %>%
  filter(!is.na(probs)) %>%
  mutate(proportion = round(n/sum(n), 5)) %>%
  data.frame()

error_rates_cat
```

Again, error rate seems to be super low: `r round(error_rates_cat[error_rates_cat$probs == TRUE,"proportion"], 3)*100`%! :)

What are the errors that did happen? We could go look at them by looking up the subject ID (`subj`), packet (`packet`), version (`version`), and question (`question`) in the following dataframe:

```{r}
errors_cat <- d_cat_check %>%
  filter(probs == TRUE) %>%
  data.frame() %>%
  select(ctry, subj, packet, version, question) %>%
  left_join(d_cat %>%
              gather(question, response, 
                     -c(ctry, subj, packet, version, batc, entr, date))) %>%
  ungroup() %>%
  distinct(ctry, subj, packet, version, batc, entr, date, question, response) %>%
  arrange(subj, packet, version, question, batc)

# look at full errors (uncomment to look, or look at errors_cat.csv)
# errors_cat
```

```{r}
# get counts per country to get proportions later
all_cat <- d_cat_check %>%
  data.frame() %>%
  select(subj, packet, version, question) %>%
  left_join(d_cat %>% 
              gather(question, response, 
                     -c(ctry, subj, packet, version, batc, entr, date))) %>%
  ungroup() %>%
  distinct(ctry, subj, packet, version, batc, entr, date, question, response) %>%
  arrange(subj, packet, version, question, batc)
```

Let's explore this just a little more (*NOTE: The counts and proportions here won't really match up with the official error rates calculated above - they kind of double-count stuff. I'll work on this in the future - for now, I think the relative proportions are still meaningful*):

- Which countries had the most errors?

```{r}
errors_cat %>%
  count(ctry) %>%
  complete(ctry, fill = list(n = 0)) %>%
  full_join(all_cat %>% count(ctry) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```


- Which packets/versions?

```{r}
errors_cat %>%
  count(packet, version) %>%
  complete(packet, version, fill = list(n = 0)) %>%
  full_join(all_cat %>% count(packet, version) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

- Which subjects?

```{r}
errors_cat %>%
  count(subj) %>%
  complete(subj, fill = list(n = 0)) %>%
  full_join(all_cat %>% count(subj) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

Which questions?

```{r}
errors_cat %>%
  count(question) %>%
  complete(question, fill = list(n = 0)) %>%
  full_join(all_cat %>% count(question) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

- Which data-enterers?
*NOTE: Of course, one person in any given mis-match is probably correct, so these aren't real error-rates - but the relative proportions might be useful.*

```{r}
errors_cat %>%
  count(entr) %>%
  complete(entr, fill = list(n = 0)) %>%
  full_join(all_cat %>% count(entr) %>% rename(total_n = n)) %>%
  mutate(proportion = round(n/total_n, 5)) %>%
  arrange(desc(proportion))
```

# Writing out error files

Finally, we'll spit out the useful error docs into .csv files.

```{r}
write.csv(errors_num, "./packets123_errors_num.csv")
write.csv(errors_cat, "./packets123_errors_cat.csv")
```


